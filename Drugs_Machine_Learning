import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

import folium
import webbrowser
from folium import plugins
from folium.plugins import FastMarkerCluster
import plotly
import plotly.graph_objs as go

from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV

from xgboost import XGBClassifier
from xgboost import plot_importance
from xgboost import cv

from wordcloud import WordCloud
from collections import Counter
from PIL import Image

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score

import shap




df = pd.read_csv('../input/UCIdrug_train.csv')
test = pd.read_csv('../input/UCIdrug_test.csv')
df.head()
df.head()

#Change date format

df = df.rename(columns={"Unnamed: 0": "uniqueID"})
df['date'] = pd.to_datetime(df['date'])

train = train.rename(columns = {"Unnamed: 0": "uniqueID"})
train['date'] = pd.to_datetime(train['date'])

#clean text
def clean_text(text):
    '''Text Preprocessing '''
    # Convert words to lower case
    text = text.lower()
    # Expand contractions
    if True:
        text = text.split()
        new_text = []
        for word in text:
            if word in contractions:
                new_text.append(contractions[word])
            else:
                new_text.append(word)
        text = " ".join(new_text)
    # Format words and remove unwanted characters
    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\<a href', ' ', text)
    text = re.sub(r'&amp;', '', text)
    text = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', text)
    text = re.sub(r'<br />', ' ', text)
    text = re.sub(r'\'', ' ', text)
    text = re.sub(r'\d+','', text)
    # remove stopwords
    if remove_stopwords:
        text = text.split()
        stops = set(stopwords.words("english"))
        text = [w for w in text if not w in stops]
        text = " ".join(text)
    # Tokenize each word
    text =  nltk.WordPunctTokenizer().tokenize(text)
    # Stemming
#     lemm = nltk.stem.WordNetLemmatizer()
#     text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))
    
    stemmer_snowball = SnowballStemmer('english')
    stem_words = []
    for w in text:
        x = stemmer_snowball.stem(w)
        stem_words.append(x)
    return stem_words


data = pd.concat([df, test])
data.head()
data.describe()
data.info()
data.dtypes
data.isnull().any()
----This barplot shows the top 20 drugs with the 10/10 rating
--The is a bar graph which shows the top 20 drugs given in the data set with a rating of 10/10. 'Levonorgestrel' is the drug with the highest number of 10/10 ratings, about 1883 Ratings in the data set for 'Levonorgestrel

sns.set(font_scale = 1.2, style = 'darkgrid')
plt.rcParams['figure.figsize'] = [15, 8]

rating = dict(data.loc[data.rating == 10, "drugName"].value_counts())
drugname = list(rating.keys())
drug_rating = list(rating.values())

sns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20])

sns_rating.set_title('Top 20 drugs with 10/10 rating')
sns_rating.set_ylabel("Number of Ratings")
sns_rating.set_xlabel("Drug Names")
plt.setp(sns_rating.get_xticklabels(), rotation=90);



sns.set(font_scale = 1.2, style = 'darkgrid')
plt.rcParams['figure.figsize'] = [15, 8]

rating = dict(data.loc[data.rating == 1, "drugName"].value_counts())
drugname = list(rating.keys())
drug_rating = list(rating.values())

sns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20], palette = 'winter')

sns_rating.set_title('Top 20 drugs with 1/10 rating')
sns_rating.set_ylabel("Number of Ratings")
sns_rating.set_xlabel("Drug Names")
plt.setp(sns_rating.get_xticklabels(), rotation=90);


--The is a bar graph thatshows the top 20 drugs given in the data set with a rating of 1/10. 'Miconazole' is the drug with the highest number of 1/10 ratings, about 767

size = [68005, 46901, 36708, 25046, 12547, 10723, 8462, 6671]
colors = ['blue', 'red', 'maroon',  'magenta', 'orange', 'navy', 'lightgreen', 'yellow']
labels = "10", "1", "9", "8", "7", "5", "6", "4"

my_circle = plt.Circle((0, 0), 0.7, color = 'white')

plt.rcParams['figure.figsize'] = (10, 10)
plt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')
plt.axis('off')
plt.title('Pie Chart Representation of Ratings', fontsize = 25)
p = plt.gcf()
plt.gca().add_artist(my_circle)
plt.legend()
plt.show()
# A countplot of the ratings so we can see the distribution of ratings
plt.rcParams['figure.figsize'] = [20,8]
sns.set(font_scale = 1.4, style = 'whitegrid')
fig, ax = plt.subplots(1, 2)

sns_1 = sns.countplot(data['rating'], palette = 'spring', order = list(range(10, 0, -1)), ax = ax[0])
sns_2 = sns.distplot(data['rating'], ax = ax[1])
sns_1.set_title('Count of Ratings')
sns_1.set_xlabel("Rating")

sns_2.set_title('Distribution of Ratings')
sns_2.set_xlabel("Rating")


# This barplot show the top 10 conditions the people are suffering.
cond = dict(data['condition'].value_counts())
top_condition = list(cond.keys())[0:10]
values = list(cond.values())[0:10]
sns.set(style = 'darkgrid', font_scale = 1.3)
plt.rcParams['figure.figsize'] = [18, 7]

sns_ = sns.barplot(x = top_condition, y = values, palette = 'winter')
sns_.set_title("Top 10 conditions")
sns_.set_xlabel("Conditions")
sns_.set_ylabel("Count");

# Top 10 drugs which are used for the top condition, that is Birth Control
df1 = data[data['condition'] == 'Birth Control']['drugName'].value_counts()[0: 10]
sns.set(font_scale = 1.2, style = 'darkgrid')

sns_ = sns.barplot(x = df1.index, y = df1.values, palette = 'summer')
sns_.set_xlabel('Drug Names')
sns_.set_title("Top 10 Drugs used for Birth Control")
plt.setp(sns_.get_xticklabels(), rotation = 90);
# let's see the words cloud for the reviews 

# most popular drugs

from wordcloud import WordCloud
from wordcloud import STOPWORDS

stopwords = set(STOPWORDS)

wordcloud = WordCloud(background_color = 'lightblue', stopwords = stopwords, width = 1200, height = 800).generate(str(data['review']))

plt.rcParams['figure.figsize'] = (15, 15)
plt.title('WORD CLOUD OF REVIEWS', fontsize = 25)
print(wordcloud)
plt.axis('off')
plt.imshow(wordcloud)
plt.show()

# let's make a new column review sentiment 

data.loc[(data['rating'] >= 5), 'Review_Sentiment'] = 1
data.loc[(data['rating'] < 5), 'Review_Sentiment'] = 0

data['Review_Sentiment'].value_counts()


# a pie chart to represent the sentiments of the patients

size = [161491, 53572]
colors = ['lightblue', 'navy']
labels = "Positive Sentiment","Negative Sentiment"
explode = [0, 0.1]

plt.rcParams['figure.figsize'] = (10, 10)
plt.pie(size, colors = colors, labels = labels, explode = explode, autopct = '%.2f%%')
plt.axis('off')
plt.title('Pie Chart Representation of Sentiments', fontsize = 25)
plt.legend()
plt.show()

# VISUALIZATION OF USEFUL COUNT
# USEFUL COUNT dist plot

plt.rcParams['figure.figsize'] = (15, 8)
sns.distplot(data['usefulCount'], color = 'orange')
plt.title('The Distribution of Useful Counts', fontsize = 30)
plt.xlabel('Range of Useful Counts', fontsize = 15)
plt.ylabel('No. of Useful Counts', fontsize = 15)
plt.show()

fig, ax = plt.subplots()
ax.scatter(train['rating'],train['review_len']);
ax.set_title("Ratings")
ax.set_xlabel("Review Length");

fig, ax = plt.subplots()
ax.scatter(train['rating'],train['usefulCount']);
ax.set_title("Ratings")
ax.set_xlabel("usefulcount");

ds=train.loc[train['rating']>5.0]
dv=train.loc[train['rating']<6.0]

df = train['date'].dt.year.value_counts(1,2,3,4,5).sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of reviews per year")
sns_.set_xlabel("Year");

df = ds['date'].dt.year.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of positive reviews per year")
sns_.set_xlabel("Year");


df = dv['date'].dt.year.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of negative reviews per year")
sns_.set_xlabel("Year");

df = dv['date'].dt.month.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of negative reviews per month")
sns_.set_xlabel("Month");


df = ds['date'].dt.month.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of positive reviews per month")
sns_.set_xlabel("Month");

df = train['date'].dt.month.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of reviews per month")
sns_.set_xlabel("Month");

######Machine Learning - Predecting rating based on review 
import pandas as pd
import numpy as np
import seaborn as sns

from matplotlib import pyplot as plt
%matplotlib inline
plt.style.use('fivethirtyeight')
params = {'legend.fontsize': 'medium',
          'figure.figsize': (10, 5),
         'axes.labelsize': 'medium',
         'axes.titlesize':'large',
         'xtick.labelsize':'medium',
         'ytick.labelsize':'medium'}
plt.rcParams.update(params)

train = pd.read_csv("../data/drugsComTrain_raw.tsv", delimiter='\t')
train = train.rename(columns = {"Unnamed: 0": "uniqueID"})
train.head()
train.shape
len(train['review'][0])
train.uniqueID.nunique()

# number of drugs
train.drugName.nunique()

# type of conditions
train.condition.nunique()

# ratings distribution
sns.set(font_scale = 1.4, style = 'whitegrid')
fig, ax = plt.subplots()
sns_1 = sns.distplot(train['rating'], ax = ax)
sns_1.set_title("Distribution of Ratings")
sns_1.set_xlabel("Rating");

train = pd.read_csv("../data/drugsComTrain_raw.tsv", delimiter='\t')
train = train.rename(columns = {"Unnamed: 0": "uniqueID"})
train['date'] = pd.to_datetime(train['date'])

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
    text: a string
    return: modified initial string
    """
    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(' ', text)
    text = BAD_SYMBOLS_RE.sub('', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    return text

train['review'] = train['review'].apply(clean_text)
train['review_len'] = train['review'].apply(lambda x: len(x))
train['review_len'].describe()

import keras
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D, LSTM
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint, EarlyStopping

#baseline model code

import pandas as pd
from collections import Counter
from sklearn.metrics import mean_absolute_error

# Load datasets
train_path = "/mnt/data/drugsComTrain_raw.tsv"
test_path = "/mnt/data/drugsComTest_raw.tsv"
train_df = pd.read_csv(train_path, sep="\t")
test_df = pd.read_csv(test_path, sep="\t")

# Find the most frequent rating in the training set
most_frequent_rating = train_df["rating"].mode()[0]

# Predict this rating for all test samples
test_df["baseline_prediction"] = most_frequent_rating

# Evaluate using Mean Absolute Error (MAE)
mae_baseline = mean_absolute_error(test_df["rating"], test_df["baseline_prediction"])

# Calculate accuracy as the percentage of exact matches
accuracy_baseline = (test_df["rating"] == test_df["baseline_prediction"]).mean() * 100

# Print results
print(f"Most Frequent Rating (Mode): {most_frequent_rating}")
print(f"Baseline MAE: {mae_baseline:.2f}")
print(f"Baseline Accuracy: {accuracy_baseline:.2f}%")



â€ƒ
# LSTM two layers

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# Load datasets
train_path = "drugsComTrain_raw.tsv"  # Update path if needed
test_path = "drugsComTest_raw.tsv"
train_df = pd.read_csv(train_path, sep="\t")
test_df = pd.read_csv(test_path, sep="\t")

# Parameters
MAX_VOCAB_SIZE = 10000  # Max words in tokenizer
MAX_SEQUENCE_LENGTH = 200  # Max review length
EMBEDDING_DIM = 100  # Embedding vector size
NUM_CLASSES = 10  # Ratings from 1 to 10

# Extract reviews and ratings
reviews_train = train_df["review"].astype(str).values
ratings_train = train_df["rating"].values

reviews_test = test_df["review"].astype(str).values
ratings_test = test_df["rating"].values

# Tokenize text
tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(reviews_train)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(reviews_train)
X_test_seq = tokenizer.texts_to_sequences(reviews_test)

# Pad sequences
X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding="post", truncating="post")
X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding="post", truncating="post")

# Convert ratings into integer class labels (1-10)
y_train = np.round(ratings_train).astype(int) - 1  # Convert to 0-9 range
y_test = np.round(ratings_test).astype(int) - 1  # Convert to 0-9 range

# One-hot encode the labels
y_train_encoded = to_categorical(y_train, num_classes=NUM_CLASSES)
y_test_encoded = to_categorical(y_test, num_classes=NUM_CLASSES)

# Build LSTM Classification Model
model = Sequential([
    Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),
    LSTM(100, return_sequences=True),
    Dropout(0.3),
    LSTM(100),
    Dropout(0.3),
    Dense(32, activation="relu"),
    Dense(NUM_CLASSES, activation="softmax")  # Multi-class output
])

# Compile model
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Train model
history = model.fit(
    X_train_padded, y_train_encoded,
    validation_data=(X_test_padded, y_test_encoded),
    epochs=5,  # Increase if needed
    batch_size=32,
    verbose=1
)

# Evaluate model
loss, accuracy = model.evaluate(X_test_padded, y_test_encoded)
print(f"Test Accuracy: {accuracy:.2f}")

# Save model
model.save("lstm_drug_rating_classifier.h5")


#accuracy visualization

import matplotlib.pyplot as plt

# Plot training & validation accuracy
plt.plot(history.history["accuracy"], label="Training Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.xlabel("Epochs")  # X-axis: Number of epochs
plt.ylabel("Accuracy")  # Y-axis: Accuracy score
plt.title("Model Accuracy Over Epochs")
plt.legend()
plt.show()


#RANDOM FOREST

import numpy as np
import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Download required NLTK data
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("stopwords")

# Load dataset
train_path = "drugsComTrain_raw.tsv"
test_path = "drugsComTest_raw.tsv"
train_df = pd.read_csv(train_path, sep="\t")
test_df = pd.read_csv(test_path, sep="\t")

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r"[^a-zA-Z]", " ", text)  # Remove special characters & numbers
    tokens = word_tokenize(text)  # Tokenize words
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatization
    return " ".join(tokens)  # Reconstruct sentence

# Apply preprocessing
train_df["clean_review"] = train_df["review"].astype(str).apply(preprocess_text)
test_df["clean_review"] = test_df["review"].astype(str).apply(preprocess_text)

# Extract reviews and ratings
X_train = train_df["clean_review"].values
y_train = np.round(train_df["rating"]).astype(int)

X_test = test_df["clean_review"].values
y_test = np.round(test_df["rating"]).astype(int)

# Convert text to TF-IDF features
vectorizer = TfidfVectorizer(max_features=10000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_tfidf, y_train)

# Predict & Evaluate
y_pred = rf_model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.2f}")
print(classification_report(y_test, y_pred))

# Top Features (Words) in Random Forest Model

import numpy as np

# Get feature names (words from TF-IDF)
feature_names = np.array(vectorizer.get_feature_names_out())

# Get feature importances from Random Forest model
feature_importances = rf_model.feature_importances_

# Sort features by importance
top_n = 20  # Show top 20 most important words
top_features_idx = np.argsort(feature_importances)[-top_n:]  # Get indices of top N words
top_features = feature_names[top_features_idx]
top_importances = feature_importances[top_features_idx]

# Display important words and their importance scores
for word, importance in zip(top_features[::-1], top_importances[::-1]):  # Reverse to show highest first
    print(f"{word}: {importance:.5f}")






#Full Code for Gradient Boosting Classification

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

# Load dataset
df = pd.read_csv("/mnt/data/drugsComTrain_raw.tsv", sep='\t')

# Keep only necessary columns
df = df[['review', 'rating']]

# Convert rating to categorical labels (multi-class classification)
df['rating'] = df['rating'].astype(int)

# Text preprocessing function
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    words = word_tokenize(text)  # Tokenization
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & stopword removal
    return " ".join(words)

# Apply preprocessing
df['clean_review'] = df['review'].apply(preprocess_text)

# Convert text to TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 words
X = vectorizer.fit_transform(df['clean_review']).toarray()
y = df['rating']

# Split dataset into training and testing sets (80-20)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Gradient Boosting Model
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train, y_train)

# Predict on test set
y_pred = gb_model.predict(X_test)

# Evaluate model performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Gradient Boosting Accuracy: {accuracy:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(10, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Blues", xticklabels=sorted(df['rating'].unique()), yticklabels=sorted(df['rating'].unique()))
plt.xlabel("Predicted Rating")
plt.ylabel("Actual Rating")
plt.title("Confusion Matrix - Gradient Boosting Model")
plt.show()

