import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math

import folium
import webbrowser
from folium import plugins
from folium.plugins import FastMarkerCluster
import plotly
import plotly.graph_objs as go

from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV

from xgboost import XGBClassifier
from xgboost import plot_importance
from xgboost import cv

from wordcloud import WordCloud
from collections import Counter
from PIL import Image

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
from sklearn.metrics import accuracy_score

import shap




df = pd.read_csv('../input/UCIdrug_train.csv')

test = pd.read_csv('../input/UCIdrug_test.csv')
df.head()
df.head()

data = pd.concat([df, test])
data.head()
data.describe()
data.info()
data.dtypes
data.isnull().any()
----This barplot shows the top 20 drugs with the 10/10 rating
--The is a bar graph which shows the top 20 drugs given in the data set with a rating of 10/10. 'Levonorgestrel' is the drug with the highest number of 10/10 ratings, about 1883 Ratings in the data set for 'Levonorgestrel

sns.set(font_scale = 1.2, style = 'darkgrid')
plt.rcParams['figure.figsize'] = [15, 8]

rating = dict(data.loc[data.rating == 10, "drugName"].value_counts())
drugname = list(rating.keys())
drug_rating = list(rating.values())

sns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20])

sns_rating.set_title('Top 20 drugs with 10/10 rating')
sns_rating.set_ylabel("Number of Ratings")
sns_rating.set_xlabel("Drug Names")
plt.setp(sns_rating.get_xticklabels(), rotation=90);



sns.set(font_scale = 1.2, style = 'darkgrid')
plt.rcParams['figure.figsize'] = [15, 8]

rating = dict(data.loc[data.rating == 1, "drugName"].value_counts())
drugname = list(rating.keys())
drug_rating = list(rating.values())

sns_rating = sns.barplot(x = drugname[0:20], y = drug_rating[0:20], palette = 'winter')

sns_rating.set_title('Top 20 drugs with 1/10 rating')
sns_rating.set_ylabel("Number of Ratings")
sns_rating.set_xlabel("Drug Names")
plt.setp(sns_rating.get_xticklabels(), rotation=90);


--The is a bar graph thatshows the top 20 drugs given in the data set with a rating of 1/10. 'Miconazole' is the drug with the highest number of 1/10 ratings, about 767

size = [68005, 46901, 36708, 25046, 12547, 10723, 8462, 6671]
colors = ['blue', 'red', 'maroon',  'magenta', 'orange', 'navy', 'lightgreen', 'yellow']
labels = "10", "1", "9", "8", "7", "5", "6", "4"

my_circle = plt.Circle((0, 0), 0.7, color = 'white')

plt.rcParams['figure.figsize'] = (10, 10)
plt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')
plt.axis('off')
plt.title('Pie Chart Representation of Ratings', fontsize = 25)
p = plt.gcf()
plt.gca().add_artist(my_circle)
plt.legend()
plt.show()
# A countplot of the ratings so we can see the distribution of ratings
plt.rcParams['figure.figsize'] = [20,8]
sns.set(font_scale = 1.4, style = 'whitegrid')
fig, ax = plt.subplots(1, 2)

sns_1 = sns.countplot(data['rating'], palette = 'spring', order = list(range(10, 0, -1)), ax = ax[0])
sns_2 = sns.distplot(data['rating'], ax = ax[1])
sns_1.set_title('Count of Ratings')
sns_1.set_xlabel("Rating")

sns_2.set_title('Distribution of Ratings')
sns_2.set_xlabel("Rating")


# This barplot show the top 10 conditions the people are suffering.
cond = dict(data['condition'].value_counts())
top_condition = list(cond.keys())[0:10]
values = list(cond.values())[0:10]
sns.set(style = 'darkgrid', font_scale = 1.3)
plt.rcParams['figure.figsize'] = [18, 7]

sns_ = sns.barplot(x = top_condition, y = values, palette = 'winter')
sns_.set_title("Top 10 conditions")
sns_.set_xlabel("Conditions")
sns_.set_ylabel("Count");

# Top 10 drugs which are used for the top condition, that is Birth Control
df1 = data[data['condition'] == 'Birth Control']['drugName'].value_counts()[0: 10]
sns.set(font_scale = 1.2, style = 'darkgrid')

sns_ = sns.barplot(x = df1.index, y = df1.values, palette = 'summer')
sns_.set_xlabel('Drug Names')
sns_.set_title("Top 10 Drugs used for Birth Control")
plt.setp(sns_.get_xticklabels(), rotation = 90);
# let's see the words cloud for the reviews 

# most popular drugs

from wordcloud import WordCloud
from wordcloud import STOPWORDS

stopwords = set(STOPWORDS)

wordcloud = WordCloud(background_color = 'lightblue', stopwords = stopwords, width = 1200, height = 800).generate(str(data['review']))

plt.rcParams['figure.figsize'] = (15, 15)
plt.title('WORD CLOUD OF REVIEWS', fontsize = 25)
print(wordcloud)
plt.axis('off')
plt.imshow(wordcloud)
plt.show()

# let's make a new column review sentiment 

data.loc[(data['rating'] >= 5), 'Review_Sentiment'] = 1
data.loc[(data['rating'] < 5), 'Review_Sentiment'] = 0

data['Review_Sentiment'].value_counts()


# a pie chart to represent the sentiments of the patients

size = [161491, 53572]
colors = ['lightblue', 'navy']
labels = "Positive Sentiment","Negative Sentiment"
explode = [0, 0.1]

plt.rcParams['figure.figsize'] = (10, 10)
plt.pie(size, colors = colors, labels = labels, explode = explode, autopct = '%.2f%%')
plt.axis('off')
plt.title('Pie Chart Representation of Sentiments', fontsize = 25)
plt.legend()
plt.show()

# VISUALIZATION OF USEFUL COUNT
# USEFUL COUNT dist plot

plt.rcParams['figure.figsize'] = (15, 8)
sns.distplot(data['usefulCount'], color = 'orange')
plt.title('The Distribution of Useful Counts', fontsize = 30)
plt.xlabel('Range of Useful Counts', fontsize = 15)
plt.ylabel('No. of Useful Counts', fontsize = 15)
plt.show()

fig, ax = plt.subplots()
ax.scatter(train['rating'],train['review_len']);
ax.set_title("Ratings")
ax.set_xlabel("Review Length");

fig, ax = plt.subplots()
ax.scatter(train['rating'],train['usefulCount']);
ax.set_title("Ratings")
ax.set_xlabel("usefulcount");

ds=train.loc[train['rating']>5.0]
dv=train.loc[train['rating']<6.0]

df = train['date'].dt.year.value_counts(1,2,3,4,5).sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of reviews per year")
sns_.set_xlabel("Year");

df = ds['date'].dt.year.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of positive reviews per year")
sns_.set_xlabel("Year");


df = dv['date'].dt.year.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of negative reviews per year")
sns_.set_xlabel("Year");

df = dv['date'].dt.month.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of negative reviews per month")
sns_.set_xlabel("Month");


df = ds['date'].dt.month.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of positive reviews per month")
sns_.set_xlabel("Month");

df = train['date'].dt.month.value_counts().sort_index()
sns_ = sns.barplot(x=df.index, y=df.values, color = 'mediumaquamarine')
sns_.set_title("Number of reviews per month")
sns_.set_xlabel("Month");

######Machine Learning - Predecting rating based on review 
import pandas as pd
import numpy as np
import seaborn as sns

from matplotlib import pyplot as plt
%matplotlib inline
plt.style.use('fivethirtyeight')
params = {'legend.fontsize': 'medium',
          'figure.figsize': (10, 5),
         'axes.labelsize': 'medium',
         'axes.titlesize':'large',
         'xtick.labelsize':'medium',
         'ytick.labelsize':'medium'}
plt.rcParams.update(params)

train = pd.read_csv("../data/drugsComTrain_raw.tsv", delimiter='\t')
train = train.rename(columns = {"Unnamed: 0": "uniqueID"})
train.head()
train.shape
len(train['review'][0])
train.uniqueID.nunique()

# number of drugs
train.drugName.nunique()

# type of conditions
train.condition.nunique()

# ratings distribution
sns.set(font_scale = 1.4, style = 'whitegrid')
fig, ax = plt.subplots()
sns_1 = sns.distplot(train['rating'], ax = ax)
sns_1.set_title("Distribution of Ratings")
sns_1.set_xlabel("Rating");

train = pd.read_csv("../data/drugsComTrain_raw.tsv", delimiter='\t')
train = train.rename(columns = {"Unnamed: 0": "uniqueID"})
train['date'] = pd.to_datetime(train['date'])

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
    text: a string
    return: modified initial string
    """
    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(' ', text)
    text = BAD_SYMBOLS_RE.sub('', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    return text

train['review'] = train['review'].apply(clean_text)
train['review_len'] = train['review'].apply(lambda x: len(x))
train['review_len'].describe()

import keras
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D, LSTM
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint, EarlyStopping

# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 50000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 200
# This is fixed.
EMBEDDING_DIM = 100
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(train['review'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))


X = tokenizer.texts_to_sequences(train['review'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

y = pd.get_dummies(train['rating']).values
print('Shape of label tensor:', y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state =42)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

epochs = 10
batch_size = 64

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])

model.summary()

accr = model.evaluate(X_test, y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

##LSTM 2nd Run

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def clean_text(text):
    """
    text: a string
    return: modified initial string
    """
    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(' ', text)
    text = BAD_SYMBOLS_RE.sub('', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    return text

train['review'] = train['review'].apply(clean_text)

import keras
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Embedding, SpatialDropout1D, LSTM
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import ModelCheckpoint, EarlyStopping


# The maximum number of words to be used. (most frequent)
MAX_NB_WORDS = 10000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 200
# This is fixed.
EMBEDDING_DIM = 100
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~', lower=True)
tokenizer.fit_on_texts(train['review'].values)
word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

X = tokenizer.texts_to_sequences(train['review'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print('Shape of data tensor:', X.shape)

y = pd.get_dummies(train['rating']).values
print('Shape of label tensor:', y.shape)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state =42)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0,return_sequences=True))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0))
model.add(Dense(10, activation='softmax'))
optimizer = keras.optimizers.Adam(lr=0.0001)
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

epochs = 10
batch_size = 64

history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)


accr = model.evaluate(X_test, y_test)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

model.summary()


plt.title('Loss')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show();

plt.title('Accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.legend()
plt.show()

y_pred = model.predict(X_test)

y_pred_label = []
y_test_label = []
for i in range(len(y_pred)):
    y_pred_label.append(labels[np.argmax(y_pred[i])])
    y_test_label.append(labels[np.argmax(y_test[i])])

from sklearn.metrics import plot_confusion_matrix, confusion_matrix

cm = confusion_matrix(y_test_label, y_pred_label)

import seaborn as sn
df_cm = pd.DataFrame(cm, index=[i for i in range(1,11)],columns = [i for i in range(1,11)])
plt.figure(figsize=(12,8))
sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')

# test on unseen data
test = pd.read_csv("../data/drugsComTest_raw.tsv", delimiter='\t')
test = test.rename(columns = {"Unnamed: 0": "uniqueID"})
test['date'] = pd.to_datetime(test['date'])
test['review'] = test['review'].apply(clean_text)
X_unseen = tokenizer.texts_to_sequences(test['review'].values)
X_unseen = pad_sequences(X_unseen, maxlen=MAX_SEQUENCE_LENGTH)
y_unseen = pd.get_dummies(test['rating']).values
y_pred_unseen = model.predict(X_unseen)
accr = model.evaluate(X_unseen, y_unseen)
print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))

y_pred_label1 = []
y_test_label1 = []
for i in range(len(y_pred_unseen)):
    y_pred_label1.append(labels[np.argmax(y_pred_unseen[i])])
    y_test_label1.append(labels[np.argmax(y_unseen[i])])
cm = confusion_matrix(y_test_label1, y_pred_label1)



df_cm = pd.DataFrame(cm, index=[i for i in range(1,11)],columns = [i for i in range(1,11)])
plt.figure(figsize=(12,8))
sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')


#RMF

import pandas as pd
import numpy as np
import seaborn as sns
import re
from nltk.corpus import stopwords
import nltk
from sklearn.metrics import mean_squared_error, r2_score

from matplotlib import pyplot as plt
%matplotlib inline
plt.style.use('fivethirtyeight')
params = {'legend.fontsize': 'medium',
          'figure.figsize': (10, 5),
         'axes.labelsize': 'medium',
         'axes.titlesize':'large',
         'xtick.labelsize':'medium',
         'ytick.labelsize':'medium'}
plt.rcParams.update(params)

	
train = pd.read_csv("../data/drugsComTrain_raw.tsv", delimiter='\t')
train = train.rename(columns = {"Unnamed: 0": "uniqueID"})
train['date'] = pd.to_datetime(train['date'])

train.head()

## baseline classification model
from sklearn.dummy import DummyClassifier
X = train.iloc[:, 0:4]
y = train['rating']
dummy_clf = DummyClassifier(strategy="most_frequent")
dummy_clf.fit(X,y)
dummy_clf.predict(X)
dummy_clf.score(X,y)

## baseline regression model
from sklearn.dummy import DummyRegressor
dummy_reg = DummyRegressor(strategy="mean")
dummy_reg.fit(X,y)
y_pred = dummy_reg.predict(X)
print("Baseline MSE: ", mean_squared_error(y, y_pred))
print("Baseline r-squared: ", dummy_reg.score(X, y))

from gensim.parsing.preprocessing import remove_stopwords
from nltk.corpus import stopwords
import nltk
from nltk.stem.snowball import SnowballStemmer
import requests, json, os, sys, time, re
from sklearn.model_selection import train_test_split, KFold
# import contractions



contractions = {
"ain't": "am not / are not / is not / has not / have not",
"aren't": "are not / am not",
"can't": "cannot",
"can't've": "cannot have",
"'cause": "because",
"could've": "could have",
"couldn't": "could not",
"couldn't've": "could not have",
"didn't": "did not",
"doesn't": "does not",
"don't": "do not",
"hadn't": "had not",
"hadn't've": "had not have",
"hasn't": "has not",
"haven't": "have not",
"he'd": "he had / he would",
"he'd've": "he would have",
"he'll": "he shall / he will",
"he'll've": "he shall have / he will have",
"he's": "he has / he is",
"how'd": "how did",
"how'd'y": "how do you",
"how'll": "how will",
"how's": "how has / how is / how does",
"I'd": "I had / I would",
"I'd've": "I would have",
"I'll": "I shall / I will",
"I'll've": "I shall have / I will have",
"I'm": "I am",
"I've": "I have",
"isn't": "is not",
"it'd": "it had / it would",
"it'd've": "it would have",
"it'll": "it shall / it will",
"it'll've": "it shall have / it will have",
"it's": "it has / it is",
"let's": "let us",
"ma'am": "madam",
"mayn't": "may not",
"might've": "might have",
"mightn't": "might not",
"mightn't've": "might not have",
"must've": "must have",
"mustn't": "must not",
"mustn't've": "must not have",
"needn't": "need not",
"needn't've": "need not have",
"o'clock": "of the clock",
"oughtn't": "ought not",
"oughtn't've": "ought not have",
"shan't": "shall not",
"sha'n't": "shall not",
"shan't've": "shall not have",
"she'd": "she had / she would",
"she'd've": "she would have",
"she'll": "she shall / she will",
"she'll've": "she shall have / she will have",
"she's": "she has / she is",
"should've": "should have",
"shouldn't": "should not",
"shouldn't've": "should not have",
"so've": "so have",
"so's": "so as / so is",
"that'd": "that would / that had",
"that'd've": "that would have",
"that's": "that has / that is",
"there'd": "there had / there would",
"there'd've": "there would have",
"there's": "there has / there is",
"they'd": "they had / they would",
"they'd've": "they would have",
"they'll": "they shall / they will",
"they'll've": "they shall have / they will have",
"they're": "they are",
"they've": "they have",
"to've": "to have",
"wasn't": "was not",
"we'd": "we had / we would",
"we'd've": "we would have",
"we'll": "we will",
"we'll've": "we will have",
"we're": "we are",
"we've": "we have",
"weren't": "were not",
"what'll": "what shall / what will",
"what'll've": "what shall have / what will have",
"what're": "what are",
"what's": "what has / what is",
"what've": "what have",
"when's": "when has / when is",
"when've": "when have",
"where'd": "where did",
"where's": "where has / where is",
"where've": "where have",
"who'll": "who shall / who will",
"who'll've": "who shall have / who will have",
"who's": "who has / who is",
"who've": "who have",
"why's": "why has / why is",
"why've": "why have",
"will've": "will have",
"won't": "will not",
"won't've": "will not have",
"would've": "would have",
"wouldn't": "would not",
"wouldn't've": "would not have",
"y'all": "you all",
"y'all'd": "you all would",
"y'all'd've": "you all would have",
"y'all're": "you all are",
"y'all've": "you all have",
"you'd": "you had / you would",
"you'd've": "you would have",
"you'll": "you shall / you will",
"you'll've": "you shall have / you will have",
"you're": "you are",
"you've": "you have"
}


def clean_text(text):
    '''Text Preprocessing '''
    # Convert words to lower case
    text = text.lower()
    # Expand contractions
    if True:
        text = text.split()
        new_text = []
        for word in text:
            if word in contractions:
                new_text.append(contractions[word])
            else:
                new_text.append(word)
        text = " ".join(new_text)
    # Format words and remove unwanted characters
    text = re.sub(r'https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\<a href', ' ', text)
    text = re.sub(r'&amp;', '', text)
    text = re.sub(r'[_"\-;%()|+&=*%.,!?:#$@\[\]/]', ' ', text)
    text = re.sub(r'<br />', ' ', text)
    text = re.sub(r'\'', ' ', text)
    text = re.sub(r'\d+','', text)
    # remove stopwords
    if remove_stopwords:
        text = text.split()
        stops = set(stopwords.words("english"))
        text = [w for w in text if not w in stops]
        text = " ".join(text)
    # Tokenize each word
    text =  nltk.WordPunctTokenizer().tokenize(text)
    # Stemming
#     lemm = nltk.stem.WordNetLemmatizer()
#     text = list(map(lambda word:list(map(lemm.lemmatize, word)), text))
    
    stemmer_snowball = SnowballStemmer('english')
    stem_words = []
    for w in text:
        x = stemmer_snowball.stem(w)
        stem_words.append(x)
    return stem_words



train['review_cleaned'] = train['review'].apply(lambda x: clean_text(x))

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import r2_score, accuracy_score

bow_transform = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[1,1], lowercase=False)
tfidf_transform = TfidfTransformer(norm=None)

X = train[['review_cleaned','usefulCount']]
y = train['rating']

X_train, X_test, y_train, y_test = train_test_split(X, y)

X_bow = bow_transform.fit_transform(X_train['review_cleaned'])
X_tfidf = tfidf_transform.fit_transform(X_bow)

X_tfidf.shape

## modeling
rmf_clf = RandomForestClassifier(max_features = 2000, class_weight="balanced")
rmf_clf.fit(X_tfidf, y_train)

X_bow_test = bow_transform.transform(X_test['review_cleaned'])
X_tfidf_test = tfidf_transform.transform(X_bow_test)


y_pred = rmf_clf.predict(X_tfidf_test)
accuracy_score(y_test, y_pred)

from sklearn.metrics import plot_confusion_matrix, confusion_matrix

cm = confusion_matrix(y_test, y_pred)

import seaborn as sn
df_cm = pd.DataFrame(cm, index=[i for i in range(1,11)],columns = [i for i in range(1,11)])
plt.figure(figsize=(12,8))
sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')

from wordcloud import WordCloud, STOPWORDS
stopwords = set(STOPWORDS)

# frequent words
text1 = " ".join(review for review in train.review)
wordcloud1 = WordCloud(width=800, height=800, background_color='white', stopwords=stopwords, min_font_size=10).generate(text1)
plt.imshow(wordcloud1)
plt.axis("off")
plt.show()

# test on unseen data
test = pd.read_csv("../data/drugsComTest_raw.tsv", delimiter='\t')
test = test.rename(columns = {"Unnamed: 0": "uniqueID"})
test['date'] = pd.to_datetime(test['date'])
test['review_cleaned'] = test['review'].apply(lambda x: clean_text(x))
X_bow_test1 = bow_transform.transform(test['review_cleaned'])
X_tfidf_test1 = tfidf_transform.transform(X_bow_test1)
y_pred1 = rmf_clf.predict(X_tfidf_test1)
y_true = test['rating']
cm = confusion_matrix(y_true, y_pred1)
print(accuracy_score(y_true, y_pred1))

# unseen data confusion matrix
df_cm = pd.DataFrame(cm, index=[i for i in range(1,11)],columns = [i for i in range(1,11)])
plt.figure(figsize=(12,8))
sn.heatmap(df_cm, annot=True, cmap='Blues', fmt='g')


